{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP90042A3T2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PnUULb4TkaOS",
        "a0V7q8tukgon",
        "FtTZAQibkrDO",
        "gFpdTfadkujP",
        "rokloE9Hk6NK",
        "Tp7_wN5ak_q2",
        "E1Cyc3tflFce",
        "0OXvvux6lbLV",
        "HLJ9Af4ylUyo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhCe5Kjbi7sB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## COMP90042 Project 2022: Rumour Detection and Analysis on Twitter\n",
        "## Task 2\n",
        "### Group Member:\n",
        "Yizhou Wang 873236 \\\n",
        "Siqi Zhou 903274 \\\n",
        "Haoyu Bai 956490 \\"
      ],
      "metadata": {
        "id": "g6JSpd8ki_OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Processing the COVID_data"
      ],
      "metadata": {
        "id": "GEXxjwjkjGZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "rumour_id = []\n",
        "non_rumour_id = []\n",
        "csv_file = open(\"covid_id_label.csv\")\n",
        "\n",
        "csvreader = csv.reader(csv_file)\n",
        "\n",
        "header = []\n",
        "header = next(csvreader)\n",
        "\n",
        "for row in csvreader:\n",
        "  if row[1] == \"1\":\n",
        "    rumour_id.append(int(row[0]))\n",
        "  else:\n",
        "    non_rumour_id.append(int(row[0]))\n",
        "\n",
        "\n",
        "\n",
        "covid_tweet = []\n",
        "\n",
        "covid_raw = open(\"covid_trimmed_data.txt\")\n",
        "\n",
        "for line in covid_raw:\n",
        "    line = line[:-1]\n",
        "    tweet_ids = line.split(\",\")\n",
        "    covid_tweet.append(tweet_ids)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def getJSONwithID(tweet_id, path):\n",
        "    str_tweet_json = \"\"\n",
        "    if (not os.path.isfile(\"./\" + path + \"/\" + tweet_id + \".json\")):\n",
        "        return {}\n",
        "    with open(path + \"/\" + tweet_id + \".json\", \"r\") as file:\n",
        "        if path == \"tweet-objects\":\n",
        "            return json.loads(file.readline())\n",
        "        else:\n",
        "            str_tweet_json = file.readline()\n",
        "    file.close()\n",
        "    str_tweet_json = str_tweet_json.replace('\\\\\"',\"\\\"\").replace('\\\\\\\\', \"\\\\\")[1:-1]\n",
        "    obj = json.loads(str_tweet_json)\n",
        "    return obj\n"
      ],
      "metadata": {
        "id": "fUFvDpr4jG0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "vmEVjbjDjJuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HashTag Analysis"
      ],
      "metadata": {
        "id": "-sksAk04jZmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### HashTag analysis\n",
        "\n",
        "rumour_hashtag_dict = {}\n",
        "for idx in rumour_id:\n",
        "    source_tweet = covid_tweet[idx][0]\n",
        "    source_tweet_json = getJSONwithID(source_tweet, \"covid_data\")\n",
        "    if \"entities\" in source_tweet_json:\n",
        "        hashtags_dict = source_tweet_json[\"entities\"][\"hashtags\"]\n",
        "        hashtags = [hashtag_dict[\"text\"] for hashtag_dict in hashtags_dict]\n",
        "        for hashtag in hashtags:\n",
        "            hashtag = hashtag.lower().replace(\"_\", \"\").replace(\"-\",\"\")\n",
        "            if hashtag not in rumour_hashtag_dict:\n",
        "                rumour_hashtag_dict[hashtag] = 1\n",
        "            else:\n",
        "                rumour_hashtag_dict[hashtag] += 1\n",
        "\n",
        "\n",
        "sorted_rumour_hashtag_dict = {k: v for k, v in sorted(rumour_hashtag_dict.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=sorted_rumour_hashtag_dict, index = [0])\n",
        "df = (df.T)\n",
        "\n",
        "df.to_excel('rumour_source_tweet.xlsx')\n",
        "\n",
        "non_rumour_hashtag_dict = {}\n",
        "for idx in non_rumour_id:\n",
        "    source_tweet = covid_tweet[idx][0]\n",
        "    source_tweet_json = getJSONwithID(source_tweet, \"covid_data\")\n",
        "    if \"entities\" in source_tweet_json:\n",
        "        hashtags_dict = source_tweet_json[\"entities\"][\"hashtags\"]\n",
        "        hashtags = [hashtag_dict[\"text\"] for hashtag_dict in hashtags_dict]\n",
        "        for hashtag in hashtags:\n",
        "            hashtag = hashtag.lower().replace(\"_\", \"\").replace(\"ãƒ¼\", \"\").replace(\"-\", \"\")\n",
        "            if hashtag not in non_rumour_hashtag_dict:\n",
        "                non_rumour_hashtag_dict[hashtag] = 1\n",
        "            else:\n",
        "                non_rumour_hashtag_dict[hashtag] += 1\n",
        "\n",
        "\n",
        "sorted_non_rumour_hashtag_dict = {k: v for k, v in sorted(non_rumour_hashtag_dict.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data=sorted_non_rumour_hashtag_dict, index = [0])\n",
        "df = (df.T)\n",
        "\n",
        "df.to_excel('non_rumour_source_tweet.xlsx') \n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "def timeFormatting(strtime):\n",
        "\n",
        "    return datetime.strptime(strtime, '%a %b %d %H:%M:%S +0000 %Y')\n",
        "\n",
        "non_rumour_time_dict = {\"Within 1 month\": 0, \"1 - 6 months\": 0, \"6  - 12 months\": 0, \"1 - 2 years\": 0, \"2 - 5 years\": 0, \"more than 5 years\": 0}\n",
        "for idx in non_rumour_id:\n",
        "    source_tweet = covid_tweet[idx][0]\n",
        "    source_tweet_json = getJSONwithID(source_tweet, \"covid_data\")\n",
        "    if \"user\" in source_tweet_json:\n",
        "        user_created = source_tweet_json[\"user\"][\"created_at\"]\n",
        "        formatted_user_created = timeFormatting(user_created)\n",
        "        tweet_created = source_tweet_json[\"created_at\"]\n",
        "        formatted_tweet_created = timeFormatting(tweet_created)\n",
        "\n",
        "        diff = formatted_tweet_created -formatted_user_created\n",
        "\n",
        "        if diff.days < 30:\n",
        "            non_rumour_time_dict[\"Within 1 month\"] += 1\n",
        "\n",
        "        elif diff.days >= 30 and diff.days < 180:\n",
        "            non_rumour_time_dict[\"1 - 6 months\"] += 1\n",
        "\n",
        "        elif diff.days >= 180 and diff.days < 365:\n",
        "            non_rumour_time_dict[\"6  - 12 months\"] += 1\n",
        "\n",
        "        elif diff.days >= 365 and diff.days < 365 * 2:\n",
        "            non_rumour_time_dict[\"1 - 2 years\"] += 1\n",
        "\n",
        "        elif diff.days >= 365 * 2 and diff.days < 365 * 5:\n",
        "            non_rumour_time_dict[\"2 - 5 years\"] += 1\n",
        "\n",
        "        else:\n",
        "            non_rumour_time_dict[\"more than 5 years\"] += 1\n",
        "\n",
        "\n",
        "print(non_rumour_time_dict)\n",
        "\n",
        "\n",
        "\n",
        "rumour_time_dict = {\"Within 1 month\": 0, \"1 - 6 months\": 0, \"6  - 12 months\": 0, \"1 - 2 years\": 0, \"2 - 5 years\": 0, \"more than 5 years\": 0}\n",
        "for idx in rumour_id:\n",
        "    source_tweet = covid_tweet[idx][0]\n",
        "    source_tweet_json = getJSONwithID(source_tweet, \"covid_data\")\n",
        "    if \"user\" in source_tweet_json:\n",
        "        user_created = source_tweet_json[\"user\"][\"created_at\"]\n",
        "        formatted_user_created = timeFormatting(user_created)\n",
        "        tweet_created = source_tweet_json[\"created_at\"]\n",
        "        formatted_tweet_created = timeFormatting(tweet_created)\n",
        "\n",
        "        diff = formatted_tweet_created -formatted_user_created\n",
        "\n",
        "        if diff.days < 30:\n",
        "            rumour_time_dict[\"Within 1 month\"] += 1\n",
        "\n",
        "        elif diff.days >= 30 and diff.days < 180:\n",
        "            rumour_time_dict[\"1 - 6 months\"] += 1\n",
        "\n",
        "        elif diff.days >= 180 and diff.days < 365:\n",
        "            rumour_time_dict[\"6  - 12 months\"] += 1\n",
        "\n",
        "        elif diff.days >= 365 and diff.days < 365 * 2:\n",
        "            rumour_time_dict[\"1 - 2 years\"] += 1\n",
        "\n",
        "        elif diff.days >= 365 * 2 and diff.days < 365 * 5:\n",
        "            rumour_time_dict[\"2 - 5 years\"] += 1\n",
        "\n",
        "        else:\n",
        "            rumour_time_dict[\"more than 5 years\"] += 1\n",
        "\n",
        "\n",
        "print(rumour_time_dict)"
      ],
      "metadata": {
        "id": "1FIWW_9qjl3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "OEvkzyJ-jZsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "from pysentimiento import create_analyzer\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n",
        "\n",
        "import csv\n",
        "rumour_id = []\n",
        "non_rumour_id = []\n",
        "csv_file = open(\"covid_id_label.csv\")\n",
        "\n",
        "csvreader = csv.reader(csv_file)\n",
        "\n",
        "header = []\n",
        "header = next(csvreader)\n",
        "\n",
        "for row in csvreader:\n",
        "  if row[1] == \"1\":\n",
        "    rumour_id.append(int(row[0]))\n",
        "  else:\n",
        "    non_rumour_id.append(int(row[0]))\n",
        "\n",
        "covid_feature = []\n",
        "\n",
        "tsv_file = open(\"feature3_covid.tsv\")\n",
        "\n",
        "tsvreader = csv.reader(tsv_file, delimiter = \"\\t\")\n",
        "\n",
        "header = []\n",
        "header = next(tsv_file)\n",
        "\n",
        "for row in tsvreader:\n",
        "  event = row[0]\n",
        "  tweets = event.split(\"[SEP]\")\n",
        "  covid_feature.append(tweets)\n",
        "\n",
        "total_sentiment_dict = {}\n",
        "index = 0\n",
        "for i in range(len(covid_feature)):\n",
        "  for j in range(len(covid_feature[i])):\n",
        "    if index % 1000 == 0:\n",
        "      print(index)\n",
        "    str_index = str(i) + \",\" + str(j)\n",
        "    tweet = covid_feature[i][j]\n",
        "    sentiment_dict = analyzer.predict(tweet).probas\n",
        "    total_sentiment_dict[str_index] = sentiment_dict\n",
        "    index += 1\n",
        "\n",
        "\n",
        "non_rumour_sentiment_dict = {\"NEU\": 0, \"POS\": 0, \"NEG\": 0}\n",
        "for i in non_rumour_id:\n",
        "  for j in range(len(covid_feature[i])):\n",
        "    if j == 1:\n",
        "      continue\n",
        "    else:\n",
        "      str_ij = str(i) + \",\" + str(j)\n",
        "      sentiment_dict = total_sentiment_dict[str_ij]\n",
        "      max_sentiment = max(sentiment_dict, key=sentiment_dict.get)\n",
        "      non_rumour_sentiment_dict[max_sentiment] += 1\n",
        "\n",
        "print(non_rumour_sentiment_dict)\n",
        "\n",
        "\n",
        "rumour_sentiment_dict = {\"NEU\": 0, \"POS\": 0, \"NEG\": 0}\n",
        "for i in rumour_id:\n",
        "  for j in range(len(covid_feature[i])):\n",
        "    if j == 1:\n",
        "      continue\n",
        "    else:\n",
        "      str_ij = str(i) + \",\" + str(j)\n",
        "      sentiment_dict = total_sentiment_dict[str_ij]\n",
        "      max_sentiment = max(sentiment_dict, key=sentiment_dict.get)\n",
        "      rumour_sentiment_dict[max_sentiment] += 1\n",
        "    \n",
        "print(rumour_sentiment_dict)"
      ],
      "metadata": {
        "id": "ohD8F00HjTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic analysis "
      ],
      "metadata": {
        "id": "7xXPGvYfkAd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### loading tweet from file"
      ],
      "metadata": {
        "id": "PnUULb4TkaOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from filehelper import *\n",
        "\n",
        "rumourlist, nonrumourlist = covidreader(\"covid_id_label.csv\", \"covid_trimmed_data.txt\")\n",
        "print(\"numeber of rumoulist = {};       number of nonrumourlist =   {}\".format(len(rumourlist),len(nonrumourlist)))"
      ],
      "metadata": {
        "id": "WAEGNm5lkEWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tweet preprocesssing\n"
      ],
      "metadata": {
        "id": "a0V7q8tukgon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from datetime import date\n",
        "# we want to anaylize how does the topic varies by time; have to sort the list by time first.\n",
        "def createtimegetter(tweet):\n",
        "    creatdate = datetime.datetime.strftime(datetime.datetime.strptime(tweet[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'), '%Y%m%d')\n",
        "    return creatdate\n",
        "\n",
        "#sorting\n",
        "rumourlist = sorted(rumourlist, key=lambda x: createtimegetter(x))\n",
        "nonrumourlist = sorted(nonrumourlist, key=lambda x: createtimegetter(x))\n",
        "\n",
        "# getting time from string representation\n",
        "def gettimefromstring(timestamp):\n",
        "    return datetime.datetime(int(timestamp[0:4]),int(timestamp[4:6]) ,int(timestamp[6:]))\n",
        "\n",
        "#seperate by 30 days periods\n",
        "def SplitByMonth(sourcelist,days = 30):\n",
        "    periodenddate = gettimefromstring(createtimegetter(sourcelist[0]))\n",
        "    intervel = datetime.timedelta(days=days)\n",
        "    periodenddate += intervel\n",
        "    chunkedsourcelist = []\n",
        "    temp = []\n",
        "    for source in sourcelist:\n",
        "        if gettimefromstring(createtimegetter(source)) < periodenddate:\n",
        "            temp.append(source)\n",
        "        else:\n",
        "            periodenddate += intervel\n",
        "            chunkedsourcelist.append(temp)\n",
        "            temp = []\n",
        "    return chunkedsourcelist\n",
        "\n",
        "nonrumourtrend = SplitByMonth(nonrumourlist)\n",
        "rumourtrend = SplitByMonth(rumourlist)\n"
      ],
      "metadata": {
        "id": "rPZixX5ykl2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "tt = TweetTokenizer()\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "stopwords = set(stopwords.words(\"english\")) \n",
        "def preprocessing(tweet,lemmatizer, stopword,tokenizer):\n",
        "    ps = PorterStemmer()\n",
        "    new_tweet = []\n",
        "    for chunk in tweet.split(\" \"):\n",
        "        if chunk.startswith('http') or chunk.startswith(\"@\") or not (re.search('[a-zA-Z]',chunk)) :\n",
        "            continue\n",
        "        chunk = re.sub(r'[^0-9a-zA-Z]+', '', chunk)\n",
        "    #tokenization\n",
        "        chunks = tokenizer.tokenize(chunk)\n",
        "        for chunk in chunks:\n",
        "        #lemmentizer\n",
        "            lemma = lemmatizer.lemmatize(chunk, \"v\")\n",
        "            if lemma == chunk:\n",
        "                lemma = lemmatizer.lemmatize(chunk, \"n\")\n",
        "            if not lemma in stopword and lemma is not None:\n",
        "                if len(lemma) > 4:\n",
        "                    new_tweet.append(lemma.lower())\n",
        "    if new_tweet != []:\n",
        "        result = ps.stem(\" \".join(new_tweet))\n",
        "        if len(result) > 4:\n",
        "            return result\n",
        "    return None\n",
        "\n",
        "rumourlist = list(filter(None,[preprocessing(tweet[\"text\"],lemmatizer,stopwords,tt) for tweet in rumourlist]))\n",
        "nonrumourlist = list(filter(None,[preprocessing(tweet[\"text\"],lemmatizer,stopwords,tt) for tweet in nonrumourlist]))"
      ],
      "metadata": {
        "id": "YBUNHZtXlrwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### hot topic analysis"
      ],
      "metadata": {
        "id": "FtTZAQibkrDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def mostcommonword(event):\n",
        "    counter1 = Counter()\n",
        "    for tweet in event:\n",
        "        if tweet != None:\n",
        "            counter1 += Counter(tweet.split(\" \"))\n",
        "    return counter1\n",
        "rumourdict = mostcommonword(rumourlist)\n",
        "nonrumourdict = mostcommonword(nonrumourlist) \n",
        "sorted_rumour= sorted(rumourdict.items(), key=lambda kv: kv[1],reverse = True)\n",
        "sorted_nonrumour= sorted(nonrumourdict.items(), key=lambda kv: kv[1],reverse = True)\n",
        "\n",
        "# check the most common 30 words\n",
        "print(sorted_rumour[:30])\n",
        "print(sorted_nonrumour[:30])\n"
      ],
      "metadata": {
        "id": "yGNdlzttkqUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### General topic anaylysis "
      ],
      "metadata": {
        "id": "gFpdTfadkujP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lda factory\n",
        "import gensim\n",
        "def ldamodel_maker(wordlist):\n",
        "    alllist =[]\n",
        "    for word in wordlist:\n",
        "        splitedword = word.split(' ')\n",
        "        alllist.append(splitedword)\n",
        "\n",
        "    dictionary = gensim.corpora.Dictionary(alllist)\n",
        "    bow_corpus = [dictionary.doc2bow(doc) for doc in alllist]\n",
        "    return gensim.models.LdaModel(bow_corpus, num_topics=9, id2word=dictionary)\n",
        "\n"
      ],
      "metadata": {
        "id": "ssmYnj43k3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# the grpah plot helper function\n",
        "def modelplot(lda_model_tfidf,isrummor):\n",
        "    rumortitle = \"rumor\" if isrummor else \"nonrumor\"\n",
        "    fig, axs = plt.subplots(3, 3,figsize=(20, 10))\n",
        "    axs = axs.flatten()\n",
        "    for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "        words = topic.split(\" + \")\n",
        "        words = [word.split(\"*\") for word in words]\n",
        "        word = [word[0] for word in words]\n",
        "        score = [word[1] for word in words]\n",
        "        ax = axs[idx]\n",
        "        ax.barh(score,word, height=0.8)\n",
        "        ax.set_title('{} tweet most common topic(tf-idf): {}'.format(rumortitle,idx))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "NAfKCOX3k4St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ploting the overall hot rumour topics "
      ],
      "metadata": {
        "id": "rokloE9Hk6NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rumourlda = ldamodel_maker(rumourlist)\n",
        "modelplot(rumourlda,True)"
      ],
      "metadata": {
        "id": "XB8BGuock_OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ploting the overall hot non - rumour topics"
      ],
      "metadata": {
        "id": "Tp7_wN5ak_q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonruorlda = ldamodel_maker(nonrumourlist)\n",
        "modelplot(nonruorlda,False)"
      ],
      "metadata": {
        "id": "sYf1gPyPlC6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trend analysis"
      ],
      "metadata": {
        "id": "E1Cyc3tflFce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pre processing the nonrumour by months\n",
        "preprocessed_nonrumortrend = []\n",
        "for sublist in nonrumourtrend:\n",
        "    preprocessed_nonrumortrend.append(list(filter(None,[preprocessing(tweet[\"text\"],lemmatizer,stopwords,tt) for tweet in sublist])))\n",
        "\n",
        "#preprocessing the rumor by months\n",
        "preprocessed_rumortrend = []\n",
        "for sublist in rumourtrend:\n",
        "    preprocessed_rumortrend.append(list(filter(None,[preprocessing(tweet[\"text\"],lemmatizer,stopwords,tt) for tweet in sublist])))\n",
        "\n",
        "#loading the lda model to a list handler\n",
        "ldamodalcontainer = []\n",
        "for nonrusublist, rusublist in zip(preprocessed_nonrumortrend,preprocessed_rumortrend):\n",
        "    ldamodalcontainer.append((ldamodel_maker(nonrusublist),ldamodel_maker(rusublist)))\n",
        "\n",
        "ldamodalcontainer = []\n",
        "for nonrusublist, rusublist in zip(preprocessed_nonrumortrend,preprocessed_rumortrend):\n",
        "    ldamodalcontainer.append((ldamodel_maker(nonrusublist),ldamodel_maker(rusublist)))"
      ],
      "metadata": {
        "id": "R9urDqiDlFBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### nonrumour topic by time"
      ],
      "metadata": {
        "id": "0OXvvux6lbLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "nonrutotal = []\n",
        "nonrutime = []\n",
        "nonrumon = 1\n",
        "for (nonrulda,rulda) in ldamodalcontainer:\n",
        "    nonru_ntopic = 1\n",
        "    for idx, topic in nonrulda.print_topics(num_words=10):\n",
        "        nonrutime.append(\"The {} mon, top {} topic\".format(nonrumon,nonru_ntopic))\n",
        "        words = topic.split(\" + \")\n",
        "        words = [word.split(\"*\") for word in words]\n",
        "        words = [word[1] for word in words]\n",
        "        nonrutotal.append(words)\n",
        "        nonru_ntopic += 1\n",
        "        \n",
        "    nonrumon = nonrumon + 1\n",
        "print(\"=========== nonrumor topic by time ======================\")\n",
        "pd.DataFrame(np.array(nonrutotal), index = nonrutime,\n",
        "                   columns=[1,2,3,4,5,6,7,8,9,10])\n",
        "                  "
      ],
      "metadata": {
        "id": "4MZTZZAylQWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### rumour topic by time"
      ],
      "metadata": {
        "id": "HLJ9Af4ylUyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rutotal = []\n",
        "rutime = []\n",
        "rumon = 1\n",
        "for (nonrulda,rulda) in ldamodalcontainer:\n",
        "    ru_ntopic = 1\n",
        "    for idx, topic in rulda.print_topics(num_words=10):\n",
        "        rutime.append(\"The {} mon, top {} topic\".format(rumon,ru_ntopic))\n",
        "        words = topic.split(\" + \")\n",
        "        words = [word.split(\"*\") for word in words]\n",
        "        words = [word[1] for word in words]\n",
        "        rutotal.append(words)\n",
        "        ru_ntopic += 1\n",
        "        \n",
        "    rumon = rumon + 1\n",
        "print(\"=========== rumor topic by time ======================\")\n",
        "pd.DataFrame(np.array(rutotal), index = rutime,\n",
        "                   columns=[1,2,3,4,5,6,7,8,9,10])"
      ],
      "metadata": {
        "id": "OBp0cQ2IlXjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User Analysis"
      ],
      "metadata": {
        "id": "g9qLkIdMl7vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OvyhwbHKmGUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_with_empty_tweet = set()\n",
        "train_raw = open(\"covid.data.txt\", \"r\")\n",
        "issue_index_train = 0\n",
        "for event in train_raw:\n",
        "    tweets = event.split(',')\n",
        "    if (not os.path.isfile(\"./covid_data/\" + tweets[0] + \".json\")):\n",
        "        index_with_empty_tweet.add(issue_index_train)\n",
        "    issue_index_train += 1\n",
        "  \n",
        "label_file = pd.read_csv(\"covid_id_label.csv\")"
      ],
      "metadata": {
        "id": "QFB_h8zrmMRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jsonloader(jsonfile):\n",
        "    try: \n",
        "        f = open(jsonfile, 'r')\n",
        "        data = json.load(f)\n",
        "        tweetobj = json.loads(data)\n",
        "        f.close()\n",
        "        return tweetobj\n",
        "    except IOError:\n",
        "        return None"
      ],
      "metadata": {
        "id": "DNCUW9bGmO1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "information_file = open(\"user_information.csv\", \"w\")\n",
        "writer = csv.writer(information_file, lineterminator='\\n')\n",
        "writer.writerow([\"tweetID\", \"userID\", \"location\", \"description\", \"followers_count\", \"friends_count\", \"count_ratio\",\n",
        "\"favourites_count\", \"is_verified\", \"time_diff\", \"label\"])\n",
        "\n",
        "with open(\"covid_id_label.csv\", newline='') as labelfile:\n",
        "    with open(\"covid_trimmed_data.txt\", newline='') as eventsfile:\n",
        "        labelreader = csv.reader(labelfile)\n",
        "        eventsreader = csv.reader(eventsfile)\n",
        "        next(labelreader)\n",
        "        for (label,events) in zip(labelreader,eventsreader):\n",
        "            #loading files\n",
        "            #templist = []\n",
        "            source = events.pop()\n",
        "            sourcetweet = jsonloader(\"covid_data/\" + source + \".json\")\n",
        "            # if there is no sourcetweet, continue\n",
        "            if sourcetweet: \n",
        "                user_id = sourcetweet[\"user\"][\"id\"]\n",
        "\n",
        "                location = sourcetweet[\"user\"][\"location\"]\n",
        "                location = re.sub(\"[^a-zA-Z0-9]+\", \" \",location)\n",
        "\n",
        "                description = sourcetweet[\"user\"][\"description\"]\n",
        "                description = re.sub(\"[^a-zA-Z0-9]+\", \" \",description)\n",
        "\n",
        "                followers_count = sourcetweet[\"user\"][\"followers_count\"]\n",
        "                friends_count = sourcetweet[\"user\"][\"friends_count\"]\n",
        "                count_ratio = 0 if friends_count == 0 else followers_count/friends_count\n",
        "                favourites_count = sourcetweet[\"user\"][\"favourites_count\"]\n",
        "                is_verified = 1 if sourcetweet[\"user\"][\"verified\"] else 0\n",
        "\n",
        "                tweet_creation_time = datetime.datetime.strptime(sourcetweet[\"created_at\"], '%a %b %d %H:%M:%S %z %Y')\n",
        "                account_creation_time = datetime.datetime.strptime(sourcetweet[\"user\"][\"created_at\"], '%a %b %d %H:%M:%S %z %Y')\n",
        "                time_diff = (tweet_creation_time - account_creation_time).days \n",
        "\n",
        "                cur_label = label[1]\n",
        "                writer.writerow([source, user_id, location, description, followers_count, friends_count, count_ratio, favourites_count, is_verified, time_diff, cur_label])\n",
        "\n",
        "            else: continue"
      ],
      "metadata": {
        "id": "MxNMMl7gmU23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_information = pd.read_csv(\"user_information.csv\")"
      ],
      "metadata": {
        "id": "hzV-CY6mmcFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "followers_count = user_information[[\"favourites_count\"]][user_information[\"label\"] == 0]\n",
        "counts, bins = np.histogram(followers_count, bins=[800, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 6000, 7000, 8000, 9000, 10000])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(bins[:-1], counts, width=np.diff(bins), edgecolor=\"black\", align=\"edge\")\n",
        "plt.xlabel(\"bins\")\n",
        "plt.ylabel(\"number\")\n",
        "plt.title(\"rumour user followers/friends\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W3EOS7S9meYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "count = user_information[\"favourites_count\"][user_information[\"label\"] == 1]\n",
        "data['bins'] = pd.cut(count,bins=[500,2000,5000,10000,100000], labels=[\"500-2000\",\"2001-5000\",\"5001-10000\",\"10000+\"])\n",
        "data = data.groupby('bins').size()"
      ],
      "metadata": {
        "id": "59YuWuGNmgVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = [\"500-2000\",\"2001-5000\",\"5001-10000\",\"10000+\"]\n",
        "title = plt.title('rumour user verified followers/friends')\n",
        "plt.pie(data, labels=label, autopct='%1.1f%%', explode=(0, 0, 0.5, 0), shadow=True)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c08wzMgRmiAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_diff = user_information[[\"time_diff\"]][user_information[\"label\"] == 1]\n",
        "counts, bins = np.histogram(time_diff, bins=[1, 10, 30, 60, 182, 365])\n",
        "\n",
        "time_diff2 = user_information[[\"time_diff\"]][user_information[\"label\"] == 0]\n",
        "counts2, bins2 = np.histogram(time_diff2, bins=[1, 10, 30, 60, 182, 365])\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "bincenters = 0.5*(bins[1:]+bins[:-1])\n",
        "plt.plot(bincenters,counts,label=\"rumour\")\n",
        "\n",
        "bincenters2 = 0.5*(bins2[1:]+bins2[:-1])\n",
        "plt.plot(bincenters2,counts2,label=\"non-rumour\")\n",
        "\n",
        "plt.xlabel(\"time_diff range\")\n",
        "plt.ylabel(\"number\")\n",
        "plt.title(\"rumour user followers/friends\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rEMfifnBmkK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "count = user_information[\"time_diff\"][user_information[\"label\"] == 1][user_information[\"is_verified\"] == 0]\n",
        "data['bins'] = pd.cut(count,bins=[1, 30, 60, 182, 365, 400])\n",
        "data = data.groupby('bins').size()"
      ],
      "metadata": {
        "id": "qpXt-q2QmmaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = [\"1-1month\", \"1-2months\", \"2-6months\", \"6months-1year\", \"1year+\"]\n",
        "title = plt.title('non-rumour user time difference')\n",
        "plt.pie(data, labels=label, autopct='%1.1f%%', shadow=True)\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NNfuze9WmoUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}