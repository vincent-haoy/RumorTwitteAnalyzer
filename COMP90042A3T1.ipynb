{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD8XpdTEYUZR"
      },
      "source": [
        "## COMP90042 Project 2022: Rumour Detection and Analysis on Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUrs0MpHhQJD"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4QoYr4YaJB"
      },
      "source": [
        "## 1. Importing Libraries and API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmWGO2pEYSuK"
      },
      "outputs": [],
      "source": [
        "## API needed for tweet retrieving\n",
        "\n",
        "import tweepy\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## Twitter API\n",
        "yizhou_API_KEY = \"\"\n",
        "yizhou_API_KEY_SECRET = \"\"\n",
        "yizhou_ACCESS_TOKEN = \"\"\n",
        "yizhou_ACCESS_TOKEN_SECRET = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVemIn_tZBiM"
      },
      "source": [
        "## 2. Retriving Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGYUhQPNZD_G"
      },
      "outputs": [],
      "source": [
        "## get train_data tweet object\n",
        "\n",
        "## get tweet object with tweet ID via API\n",
        "\n",
        "## access API\n",
        "auth = tweepy.OAuthHandler(yizhou_API_KEY, yizhou_API_KEY_SECRET)\n",
        "auth.set_access_token(yizhou_ACCESS_TOKEN, yizhou_ACCESS_TOKEN_SECRET)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# ## test if the API works\n",
        "trainID = \"1250219300389974016\"\n",
        "tweetFetched = api.get_status(trainID)\n",
        "\n",
        "## read train_tweetID\n",
        "train_raw = open(\"train.data.txt\", \"r\")\n",
        "train_event_list = []\n",
        "for event in train_raw:\n",
        "    event = event[:-1] ## get rid of \\n in each line\n",
        "    train_event_list.append(event)\n",
        "\n",
        "## write fetched tweet_object in JSON\n",
        "tweet_with_issue = {}\n",
        "for event in train_event_list:\n",
        "    tweet_list = event.split(\",\")\n",
        "    for tweet in tweet_list:\n",
        "        if (not os.path.isfile(\"./train_data/\" + tweet + \".json\")):\n",
        "            try:\n",
        "                tweet_object = api.get_status(tweet)._json\n",
        "                tweet_object_string = json.dumps(tweet_object)\n",
        "                with open(\"train_data/\" + tweet+\".json\", \"w\") as outfile:\n",
        "                    json.dump(tweet_object_string, outfile)\n",
        "            except tweepy.errors.NotFound:\n",
        "                tweet_with_issue[tweet] = \"tweet not found\"\n",
        "                print(\"not found\")\n",
        "            except tweepy.errors.Forbidden:\n",
        "                tweet_with_issue[tweet] = \"user suspended\"\n",
        "                print(\"user suspended\")\n",
        "            except Exception as e:\n",
        "                tweet_with_issue[tweet] = \"other issue\"\n",
        "                print(e)\n",
        "\n",
        "\n",
        "with open('tweet_with_issue.txt', 'w') as twi:\n",
        "    twi.write(str(tweet_with_issue))\n",
        "\n",
        "tweet_with_issue = open(\"tweet_with_issue.txt\", \"r\")\n",
        "tweet_dictionary = json.loads(tweet_with_issue.read().replace(\"\\'\", \"\\\"\"))\n",
        "\n",
        "for tweet in tweet_dictionary:\n",
        "    if tweet_dictionary[tweet] == \"other issue\":\n",
        "        tweet_object = api.get_status(tweet)._json\n",
        "        tweet_object_string = json.dumps(tweet_object)\n",
        "        with open(\"train_data/\" + tweet+\".json\", \"w\") as outfile:\n",
        "            json.dump(tweet_object_string, outfile)\n",
        "\n",
        "\n",
        "### Retrieve json needed for task 2.\n",
        "\n",
        "## read train_tweetID\n",
        "covid_raw = open(\"covid.data.txt\", \"r\")\n",
        "line = 0\n",
        "covid_event_list = []\n",
        "for event in covid_raw:\n",
        "    event = event[:-1] ## get rid of \\n in each line\n",
        "    covid_event_list.append(event)\n",
        "    line += 1\n",
        "\n",
        "## write fetched tweet_object in JSON\n",
        "tweet_count = 0\n",
        "tweet_with_issue = {}\n",
        "for i in range(8735):\n",
        "    tweet_list = covid_event_list[i].split(\",\")\n",
        "    for tweet in tweet_list:\n",
        "        tweet_count += 1\n",
        "        if (not os.path.isfile(\"./covid_data/\" + tweet + \".json\")):\n",
        "            try:\n",
        "                tweet_object = api.get_status(tweet)._json\n",
        "                tweet_object_string = json.dumps(tweet_object)\n",
        "                with open(\"covid_data/\" + tweet+\".json\", \"w\") as outfile:\n",
        "                    json.dump(tweet_object_string, outfile)\n",
        "                    print(str(float(tweet_count/126574)), tweet, \"success\")\n",
        "            except tweepy.errors.NotFound:\n",
        "                tweet_with_issue[tweet] = \"tweet not found\"\n",
        "                print(str(float(tweet_count/126574)), tweet, \"not found\")\n",
        "            except tweepy.errors.Forbidden:\n",
        "                tweet_with_issue[tweet] = \"user suspended\"\n",
        "                print(str(float(tweet_count/126574)), tweet, \"user suspended\")\n",
        "            except Exception as e:\n",
        "                tweet_with_issue[tweet] = \"other issue\"\n",
        "                print(str(float(tweet_count/126574)), tweet, str(e))\n",
        "\n",
        "\n",
        "with open('covid_with_issue.txt', 'w') as twi:\n",
        "    twi.write(str(tweet_with_issue))\n",
        "\n",
        "\n",
        "# to retrieve again the tweets with internet issue last time\n",
        "\n",
        "covid_dictionary = open(\"covid_with_issue.txt\", \"r\")\n",
        "dict_str = covid_dictionary.readline()\n",
        "dict_str = dict_str.replace(\"\\'\", \"\\\"\")\n",
        "res = json.loads(dict_str)\n",
        "\n",
        "count = 0\n",
        "for tweet in res:\n",
        "    if res[tweet] == \"other issue\":\n",
        "        try:\n",
        "            tweet_object = api.get_status(tweet)._json\n",
        "            tweet_object_string = json.dumps(tweet_object)\n",
        "            with open(\"covid_data/\" + tweet+\".json\", \"w\") as outfile:\n",
        "                json.dump(tweet_object_string, outfile)\n",
        "        except tweepy.errors.NotFound:\n",
        "            print(\"not found\")\n",
        "        except tweepy.errors.Forbidden:\n",
        "            print(\"user suspended\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"other issue\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqLhDLNMZXlC"
      },
      "source": [
        "## 3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sKPqpWZZbq4"
      },
      "outputs": [],
      "source": [
        "# we need to skip the events with empty source tweet (user suspended/ tweet not found)\n",
        "index_with_empty_tweet_train = set()\n",
        "index_with_empty_tweet_dev = set()\n",
        "index_with_empty_tweet_covid = set()\n",
        "\n",
        "train_raw = open(\"train.data.txt\", \"r\")\n",
        "issue_index_train = 0\n",
        "for event in train_raw:\n",
        "    tweets = event[:-1].split(',')\n",
        "    if (not os.path.isfile(\"./train_data/\" + tweets[0] + \".json\")):\n",
        "        index_with_empty_tweet_train.add(issue_index_train)\n",
        "    issue_index_train += 1\n",
        "dev_raw = open(\"dev.data.txt\", \"r\")\n",
        "issue_index_dev = 0\n",
        "for event in dev_raw:\n",
        "    tweets = event[:-1].split(',')\n",
        "    if (not os.path.isfile(\"./dev_data/\" + tweets[0] + \".json\")):\n",
        "        index_with_empty_tweet_dev.add(issue_index_dev)\n",
        "    issue_index_dev += 1\n",
        "\n",
        "covid_raw = open(\"covid.data.txt\", \"r\")\n",
        "issue_index_covid = 0\n",
        "for event in covid_raw:\n",
        "    tweets = event[:-1].split(',')\n",
        "    if (not os.path.isfile(\"./covid_data/\" + tweets[0] + \".json\")):\n",
        "        index_with_empty_tweet_covid.add(issue_index_covid)\n",
        "    issue_index_covid += 1\n",
        "print(len(index_with_empty_tweet_train))\n",
        "print(len(index_with_empty_tweet_dev))\n",
        "\n",
        "def getJSONwithID(tweet_id, path):\n",
        "    str_tweet_json = \"\"\n",
        "    if (not os.path.isfile(\"./\" + path + \"/\" + tweet_id + \".json\")):\n",
        "        return {}\n",
        "    with open(path + \"/\" + tweet_id + \".json\", \"r\") as file:\n",
        "        if path == \"tweet-objects\":\n",
        "            return json.loads(file.readline())\n",
        "        else:\n",
        "            str_tweet_json = file.readline()\n",
        "    file.close()\n",
        "    str_tweet_json = str_tweet_json.replace('\\\\\"',\"\\\"\").replace('\\\\\\\\', \"\\\\\")[1:-1]\n",
        "    obj = json.loads(str_tweet_json)\n",
        "\n",
        "        \n",
        "    return obj\n",
        "\n",
        "def sortTweetsByTime(tweet_list, path):\n",
        "    unsorted_tweet_list = []\n",
        "    for tweet_ID in tweet_list:\n",
        "        tweet_json = getJSONwithID(tweet_ID, path)\n",
        "        if \"created_at\" in tweet_json:\n",
        "            formatted_created_at = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet_json[\"created_at\"], '%a %b %d %H:%M:%S +0000 %Y'))\n",
        "            unsorted_tweet_list.append((tweet_ID, formatted_created_at))\n",
        "    sorted_tweet_list_tuple = sorted(unsorted_tweet_list, key=lambda x: x[1])\n",
        "    sorted_tweet_list = []\n",
        "    for tweet_id, formatted_created_at in sorted_tweet_list_tuple:\n",
        "        sorted_tweet_list.append(tweet_id)\n",
        "    return sorted_tweet_list\n",
        "            \n",
        "# featureONE: only the source tweet text\n",
        "def getFeatureONE(filename, path, funcList, issue_index):\n",
        "    source_tweet_list = []\n",
        "    file = open(filename+\".txt\", \"r\")## train.data\n",
        "    index = 0\n",
        "    for event in file:\n",
        "        if index in issue_index:\n",
        "            index += 1\n",
        "            continue\n",
        "        event = event[:-2]\n",
        "        tweet_list = event.split(',')\n",
        "        source_tweet_id = tweet_list[0]\n",
        "        source_tweet_json = getJSONwithID(source_tweet_id, path)\n",
        "        if \"text\" not in source_tweet_json:\n",
        "            source_tweet_text = \"\"\n",
        "        else:\n",
        "            source_tweet_text = source_tweet_json[\"text\"]\n",
        "        source_tweet_list.append(engineerTweet(source_tweet_text, funcList))\n",
        "        index += 1\n",
        "    file.close()\n",
        "    return source_tweet_list\n",
        "\n",
        "# featureTWO: all tweets in the event\n",
        "def getFeatureTWO(filename, path, funcList, issue_index):\n",
        "    tweets_list = []\n",
        "    file = open(filename+\".txt\", \"r\")\n",
        "    index = 0\n",
        "    for event in file:\n",
        "        if index in issue_index:\n",
        "            index += 1\n",
        "            continue\n",
        "        event = event[:-1]\n",
        "        tweet_txt_list = []\n",
        "        tweet_list = event.split(',')\n",
        "        for tweet_id in tweet_list:\n",
        "            tweet_json = getJSONwithID(tweet_id, path)\n",
        "            if \"text\" not in tweet_json:\n",
        "                continue\n",
        "            else:\n",
        "                tweet_text = tweet_json[\"text\"]\n",
        "            tweet_txt_list.append(engineerTweet(tweet_text, funcList))\n",
        "        tweets_list.append(tweet_txt_list)\n",
        "        index += 1\n",
        "    file.close()\n",
        "    return tweets_list\n",
        "\n",
        "# featureTHREE: get source tweet and the last x replies in the event\n",
        "def getFeatureTHREE(filename, path, noOfReplies, funcList, issue_index):\n",
        "    tweets_list = []\n",
        "    file = open(filename+\".txt\", \"r\")\n",
        "    index = 0\n",
        "    for event in file:\n",
        "        if index in issue_index:\n",
        "            index += 1\n",
        "            continue\n",
        "        tweet_txt_list = []\n",
        "        event = event[:-1]\n",
        "        tweet_list = event.split(',')\n",
        "        source_tweet_id = tweet_list[0]\n",
        "        source_tweet_json = getJSONwithID(source_tweet_id, path)\n",
        "        if \"text\" not in source_tweet_json:\n",
        "            tweet_txt_list.append(\"\")\n",
        "        else:\n",
        "            tweet_txt_list.append(engineerTweet(source_tweet_json[\"text\"], funcList))\n",
        "        if len(tweet_list) > 1:\n",
        "            reply_tweet_id_list = tweet_list[1:]\n",
        "            sorted_reply_tweet_id_list = sortTweetsByTime(reply_tweet_id_list, path)\n",
        "            if len(sorted_reply_tweet_id_list) > noOfReplies:\n",
        "                sorted_reply_tweet_id_list = sorted_reply_tweet_id_list[-1 * noOfReplies: ]\n",
        "            for tweet_id in sorted_reply_tweet_id_list:\n",
        "                reply_txt = engineerTweet(getJSONwithID(tweet_id, path)[\"text\"], funcList)\n",
        "                tweet_txt_list.append(reply_txt)\n",
        "        tweets_list.append(tweet_txt_list)\n",
        "        index += 1\n",
        "    return tweets_list\n",
        "\n",
        "\n",
        "\n",
        "def engineerTweet(tweet, functionList):\n",
        "    for func in functionList:\n",
        "        tweet = func(tweet)\n",
        "    return tweet\n",
        "\n",
        "## feature engineering\n",
        "def defaultEngineering(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r' {2,}',' ', tweet)\n",
        "    return tweet.replace(\"\\n\", \" \")\n",
        "\n",
        "def removeStopWords(tweet):\n",
        "    tweet_list = tweet.split()\n",
        "    return ' '.join([i for i in tweet_list if i not in stop_words])\n",
        "\n",
        "def removeURL(tweet):\n",
        "    return re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', 'HTTP', tweet)\n",
        "\n",
        "def removeAtMentions(tweet):\n",
        "    return re.sub(r'@[A-Za-z0-9_]+', '@USER', tweet)\n",
        "\n",
        "def removePunctuationMarks(tweet):\n",
        "    return re.sub(r'[^a-zA-Z#]', ' ', tweet)\n",
        "\n",
        "def shortenDuplicatedLetters(tweet):\n",
        "    return re.sub(r'([a-z])\\1+', r'\\1'r'\\1', tweet) # replace 'coool' with 'cool' (more than 3 repeats to 2)\n",
        "\n",
        "def removeNonAlphaNumericCharacters(tweet):\n",
        "    return re.sub(\"[^a-z0-9]\",\" \", tweet)\n",
        "\n",
        "def removeDuplicatedSpace(tweet):\n",
        "    tweet = re.sub(r' {2,}',' ', tweet)\n",
        "    if tweet == \"\":\n",
        "        return tweet\n",
        "    if tweet[0] == \" \":\n",
        "        tweet = tweet[1:]\n",
        "    if tweet == \"\":\n",
        "        return tweet\n",
        "    if tweet[-1] == \" \":\n",
        "        tweet = tweet[:-1]\n",
        "    return tweet\n",
        "    \n",
        "\n",
        "origin_tweet = \"Hao RT @WSJ: Breaking: Microsoft is near a deal to buy Minecraft maker Mojang  http://t.co/2S6t0Hvy28\"\n",
        "processed_tweet = engineerTweet(origin_tweet, [defaultEngineering, removeStopWords, removeAtMentions, removeURL, removePunctuationMarks, shortenDuplicatedLetters])\n",
        "print(processed_tweet)\n",
        "\n",
        "def getLabel(filename, issue_index):\n",
        "    label_list = []\n",
        "    file = open(filename + \".txt\", \"r\")\n",
        "    index = 0\n",
        "    for label in file:\n",
        "        if index in issue_index:\n",
        "            index += 1\n",
        "            continue\n",
        "        label_list.append(label[:-1])\n",
        "        index += 1\n",
        "    return label_list\n",
        "\n",
        "\n",
        "def writeToTxt(string_list, filename):\n",
        "    with open(\"./data_txt/\" + filename + \".txt\", 'w') as f:\n",
        "        for string in string_list:\n",
        "            f.write(string)\n",
        "            f.write('\\n')\n",
        "        f.close()\n",
        "\n",
        "def writeToTSV(X, y, filename):\n",
        "    with open(filename + \".tsv\", \"wt\") as out_file:\n",
        "        tsv_writer = csv.writer(out_file, delimiter=\"\\t\")\n",
        "        tsv_writer.writerow(['sentence', 'label'])\n",
        "        for i in range(len(X)):\n",
        "            if y[i] == \"rumour\":\n",
        "                tsv_writer.writerow([X[i], 1])\n",
        "            else:\n",
        "                tsv_writer.writerow([X[i], 0])\n",
        "    out_file.close()\n",
        "\n",
        "def twoDListToList(li):\n",
        "    output_list = []\n",
        "    for i in range(len(li)):\n",
        "        long_string = listToString(li[i])\n",
        "        output_list.append(long_string)\n",
        "    return output_list\n",
        "\n",
        "\n",
        "def listToString(li):\n",
        "    long_string = \" [SEP] \"\n",
        "    return (long_string.join(li))\n",
        "\n",
        "    \n",
        "funcList = [removeAtMentions, removeURL]\n",
        "\n",
        "X_train_feature1 = getFeatureONE(\"train.data\", \"train_data\", funcList, index_with_empty_tweet_train)\n",
        "X_train_feature2 = getFeatureTWO(\"train.data\", \"train_data\", funcList, index_with_empty_tweet_train)\n",
        "X_train_feature3 = getFeatureTHREE(\"train.data\", \"train_data\", 500, funcList, index_with_empty_tweet_train)\n",
        "y_train = getLabel(\"train.label\", index_with_empty_tweet_train)\n",
        "\n",
        "X_train_feature2_x = twoDListToList(X_train_feature2)\n",
        "X_train_feature3_x = twoDListToList(X_train_feature3)\n",
        "\n",
        "X_dev_feature1 = getFeatureONE(\"dev.data\", \"dev_data\", funcList, index_with_empty_tweet_dev)\n",
        "X_dev_feature2 = getFeatureTWO(\"dev.data\", \"dev_data\", funcList, index_with_empty_tweet_dev)\n",
        "X_dev_feature3 = getFeatureTHREE(\"dev.data\", \"dev_data\", 500, funcList, index_with_empty_tweet_dev)\n",
        "y_dev = getLabel(\"dev.label\", index_with_empty_tweet_dev)\n",
        "\n",
        "\n",
        "X_covid_feature3 = getFeatureTHREE(\"covid.data\", \"covid_data\", 500, funcList, index_with_empty_tweet_covid)\n",
        "y_covid = [99] * len(X_covid_feature3)\n",
        "\n",
        "X_covid_feature3_x = twoDListToList(X_covid_feature3)\n",
        "writeToTSV(X_covid_feature3_x, y_covid, \"feature3_covid\")\n",
        "\n",
        "\n",
        "\n",
        "y_dev_1 = []\n",
        "for y in y_dev:\n",
        "    if y == \"nonrumour\":\n",
        "        y_dev_1.append(0)\n",
        "    else:\n",
        "        y_dev_1.append(1)\n",
        "print(len(y_dev_1))\n",
        "print(y_dev_1)\n",
        "\n",
        "X_dev_feature2_x = twoDListToList(X_dev_feature2)\n",
        "X_dev_feature3_x = twoDListToList(X_dev_feature3)\n",
        "\n",
        "X_test_feature1 = getFeatureONE(\"test.data\", \"tweet-objects\", funcList, set())\n",
        "X_test_feature2 = getFeatureTWO(\"test.data\", \"tweet-objects\", funcList, set())\n",
        "X_test_feature3 = getFeatureTHREE(\"test.data\", \"tweet-objects\", 500, funcList, set())\n",
        "\n",
        "X_test_feature2_x = twoDListToList(X_test_feature2)\n",
        "X_test_feature3_x = twoDListToList(X_test_feature3)\n",
        "y_test = [99] * len(X_test_feature1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "writeToTSV(X_train_feature1, y_train, \"feature1_train\")\n",
        "writeToTSV(X_train_feature2_x, y_train, \"feature2_train\")\n",
        "writeToTSV(X_train_feature3_x, y_train, \"feature3_train\")\n",
        "writeToTSV(X_dev_feature1, y_dev, \"feature1_dev\")\n",
        "writeToTSV(X_dev_feature2_x, y_dev, \"feature2_dev\")\n",
        "writeToTSV(X_dev_feature3_x, y_dev, \"feature3_dev\")\n",
        "writeToTSV(X_test_feature1, y_test, \"feature1_test\")\n",
        "writeToTSV(X_test_feature2_x, y_test, \"feature2_test\")\n",
        "writeToTSV(X_test_feature3_x, y_test, \"feature3_test\")\n",
        "assert(len(X_train_feature1) == 1895 - 330)\n",
        "assert(len(X_train_feature2) == 1895 - 330)\n",
        "assert(len(X_train_feature3) == 1895 - 330)\n",
        "assert(len(y_train) == 1895 - 330)\n",
        "assert(len(X_dev_feature1) == 632 - 95)\n",
        "assert(len(X_dev_feature2) == 632 - 95)\n",
        "assert(len(X_dev_feature3) == 632 - 95)\n",
        "assert(len(y_dev) == 632 - 95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8EWuDy5dji8"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USlFC805dsB4"
      },
      "source": [
        "\n",
        "## initializer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5MqQP_Ndi6x"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "class sstdataset(Dataset):\n",
        "    def __init__(self, maxlen,tokernizertype ='bert-base-uncased'):\n",
        "        self.maxlen = maxlen\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokernizertype,\n",
        "        num_labels = 2\n",
        "        )\n",
        "        self.ATTNMASK_ARRAY = []\n",
        "        self.TOKEN_IDS = []\n",
        "        self.LABEL = []\n",
        "\n",
        "    def bert_sentences_processing(self,sentences,label):\n",
        "        SegID = 0\n",
        "        token = ['[CLS]']\n",
        "        SegIDs = []\n",
        "        token_len = 1\n",
        "        for sentence in sentences:\n",
        "            current_token = self.tokenizer.tokenize(sentence)\n",
        "            len_of_current_token = len(current_token)\n",
        "            token_len += (len_of_current_token + 1)\n",
        "            token += current_token + ['[SEP]']\n",
        "            SegIDs.extend((len_of_current_token + 1 ) * [SegID]) \n",
        "            SegID += 1\n",
        "        if int(token_len) < int(self.maxlen):\n",
        "            token = token + ['[PAD]' for _ in range(self.maxlen - token_len)]\n",
        "        else:\n",
        "            if token[self.maxlen-2] == '[SEP]':\n",
        "                token = token[:self.maxlen-1] + ['[PAD]']\n",
        "            else:\n",
        "                token = token[:self.maxlen-1] + ['[SEP]']\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(token)\n",
        "        attn_mask = [int(num != 0) for num in token_ids]\n",
        "        self.TOKEN_IDS.append(token_ids)\n",
        "        self.ATTNMASK_ARRAY.append(attn_mask)\n",
        "        self.LABEL.append(label)\n",
        "    def __len__(self):\n",
        "        return len(self.LABEL)\n",
        "    def __getitem__(self,index):\n",
        "        return torch.tensor(self.TOKEN_IDS[index]), torch.tensor(self.ATTNMASK_ARRAY[index]),torch.tensor(self.LABEL[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWUn2dead4Jr"
      },
      "source": [
        "## Constructing dataset for test, train, and dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pGP4cfseCNq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#paramter for dataloaders\n",
        "DATABASESIZE = 130\n",
        "BATCHSIZE = 1\n",
        "NUMBEROFWORKER = 0\n",
        "\n",
        "# construction\n",
        "BERTtrainDataset = sstdataset(DATABASESIZE)\n",
        "BertDEVDataset = sstdataset(DATABASESIZE)\n",
        "BertOverallDataset = sstdataset(DATABASESIZE)\n",
        "BertTESTDataset = sstdataset(DATABASESIZE)\n",
        "\n",
        "# loading from the preprocessed text\n",
        "\n",
        "#adding training set\n",
        "for addxtrain, addytrain in zip(X_train_feature3,y_train):\n",
        "    if(addxtrain):\n",
        "        BERTtrainDataset.bert_sentences_processing(addxtrain,addytrain)\n",
        "#adding testing data\n",
        "for addxtrain, addytrain in zip(X_dev_feature3,y_dev):\n",
        "    if(addxtrain):\n",
        "        BertDEVDataset.bert_sentences_processing(addxtrain,addytrain)\n",
        "# loading the test dataset\n",
        "yyindex = 0\n",
        "for addxtest in X_test_feature3:\n",
        "    BertTESTDataset.bert_sentences_processing(addxtest,yyindex)\n",
        "    yyindex += 1\n",
        "    \n",
        "\n",
        "#create data loader for the dataset objects\n",
        "train_loader = DataLoader(BERTtrainDataset, batch_size = BATCHSIZE, num_workers = NUMBEROFWORKER)\n",
        "dev_loader = DataLoader(BertDEVDataset, batch_size = BATCHSIZE, num_workers = NUMBEROFWORKER)\n",
        "test_loader = DataLoader(BertTESTDataset, batch_size = BATCHSIZE, num_workers = NUMBEROFWORKER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhacPz5beGsD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9ixVwFWZhG9"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDrDCzAmewgO"
      },
      "source": [
        "### BERT_FNN_Single Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNJXIYRtZkbT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import BertModel\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        #Instantiating BERT model object \n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "\n",
        "        # #Obtaining the representation of [CLS] head (the first token)\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        # #Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "        return logits\n",
        "\n",
        "gpu = 0 #gpu ID\n",
        "net = SentimentClassifier()\n",
        "net.cuda(gpu) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        net.train()\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "              \n",
        "            if it % 100 == 0:\n",
        "                \n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
        "                st = time.time()\n",
        "\n",
        "        \n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "\n",
        "num_epoch = 10\n",
        "\n",
        "#fine-tune the model\n",
        "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def get_predictions(model, loader):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    probs_all = []\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, label in loader:\n",
        "            seq, attn_masks = seq.cuda(gpu), attn_masks.cuda(gpu)\n",
        "            logits = model(seq, attn_masks)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "            prob = [prob for prob in probs]\n",
        "            probs_all += prob\n",
        "            soft_probs = [int(prob > 0.5) for prob in probs]\n",
        "            predictions += soft_probs\n",
        "    return [predictions, probs_all]\n",
        "\n",
        "import csv\n",
        "filename = \"covid_results.csv\"\n",
        "#y_dev = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
        "pre = get_predictions(net,covid_loader)\n",
        "with open(filename,'w',newline='') as f:\n",
        "    csvwritter = csv.writer(f,delimiter = ',')\n",
        "    csvwritter.writerow([\"Id\",\"Predicted\",\"prob\"])\n",
        "    for i in range(len(pre[0])):\n",
        "      csvwritter.writerow([i, pre[0][i], pre[1][i]])\n",
        "\n",
        "# pre = get_predictions(net,dev_loader)\n",
        "# y_dev = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
        "# print(classification_report(y_dev, pre, digits=5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRl0XkLugBVT"
      },
      "source": [
        "### Multi-layers FNN with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCw0zcCBgMhR"
      },
      "outputs": [],
      "source": [
        "################################### MODLE CONSTRUCTION #################################\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, get_linear_schedule_with_warmup\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeedforwardNeuralNetModel, self).__init__()\n",
        "    HIDDENDIMENTION = 256\n",
        "    self.bert_layer = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "    self.fc1 = nn.Linear(768, HIDDENDIMENTION)\n",
        "    self.fc2 = nn.Linear(HIDDENDIMENTION,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, seq, attn_masks):\n",
        "        # Linear function  # LINEAR\n",
        "      outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
        "      cont_reps = outputs.last_hidden_state\n",
        "      cls_rep = cont_reps[:, 0]\n",
        "      # into hidden layers\n",
        "      x = F.relu(self.fc1(cls_rep))\n",
        "      x = self.fc2(x)\n",
        "      x = self.sigmoid(x)\n",
        "      return x\n",
        "\n",
        "FNNgpu = 0\n",
        "FNNnet = FeedforwardNeuralNetModel()\n",
        "FNNnet.cuda(FNNgpu)\n",
        "FNNcriterion =  nn.BCELoss()\n",
        "FNNoptimizer = torch.optim.AdamW(FNNnet.parameters(), lr=2e-5, weight_decay=0.97)\n",
        "\n",
        "#################################### training ##############################################\n",
        "def FNNtrain(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    total_steps = len(train_loader) * max_eps\n",
        "    for ep in range(max_eps):\n",
        "        net.train()\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq = seq.cuda(gpu)\n",
        "            attn_masks = attn_masks.cuda(gpu)\n",
        "            labels = labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            loss = criterion(logits, labels.unsqueeze(1).float())\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            \n",
        "            if it % 200 == 0:\n",
        "                print(\"Iteration {} of epoch {} Time taken (s): {}, loss =  {}\".format(it, ep, (time.time()-st),loss.item()))\n",
        "                st = time.time() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5XEFVB9gULn"
      },
      "outputs": [],
      "source": [
        "# fine- tuning\n",
        "FNNtrain(FNNnet, FNNcriterion, FNNoptimizer, overall_loader, dev_loader, 3, FNNgpu)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap7Zc0z_fKEZ"
      },
      "source": [
        "### Logestic Regression Classifier with last 4 layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1qYHwrIaFiV"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased',\n",
        "        num_labels = 2\n",
        "        )\n",
        "        #768 for base; 1024 for large\n",
        "        self.lin = nn.Linear(3072, 1)\n",
        "\n",
        "\n",
        "    def forward(self, seq, attn_masks):        \n",
        "        cls_hs  = self.bert_layer(seq, attention_mask = attn_masks, return_dict= True,  output_hidden_states=True)\n",
        "        encoded_layers = cls_hs['hidden_states'][-4:]\n",
        "        concatenated = torch.cat(encoded_layers, -1)\n",
        "        logits = self.lin(concatenated[:, 0])\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# assinging a gpu to the model \n",
        "gpu = 0\n",
        "net = LogisticRegression()\n",
        "net.cuda(gpu)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "    import time\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        net.train()\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq = seq.cuda(gpu)\n",
        "            attn_masks = attn_masks.cuda(gpu)\n",
        "            labels = labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            if it % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
        "                st = time.time()\n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))\n",
        "num_epoch = 11\n",
        "\n",
        "#fine-tune the model\n",
        "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzUx9w8nfhzy"
      },
      "source": [
        "### Convulution Classifier with last 4 bert layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcF2QI97aFtm"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, get_linear_schedule_with_warmup\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch\n",
        "#loss_fn = nn.CrossEntropyLoss()\n",
        "class CNNBert(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, bert_model='bert-base-uncased'):\n",
        "        super(CNNBert, self).__init__()\n",
        "        self.bert_layer = BertModel.from_pretrained(bert_model, output_hidden_states=True\n",
        "        )\n",
        "        filter_sizes = [1,2,3,4,5]\n",
        "        num_filters = 32\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(4, num_filters, (K, embed_size)) for K in filter_sizes])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(len(filter_sizes)*num_filters, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.bert_model = bert_model\n",
        "\n",
        "    def forward(self, seq, attn_masks):   \n",
        "        x = self.bert_layer(seq, attention_mask = attn_masks, output_hidden_states=True)[2][-4:]\n",
        "        x = torch.stack(x, dim=1)\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] \n",
        "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        logit = self.fc1(x)\n",
        "        \n",
        "        return self.sigmoid(logit)\n",
        "CNNgpu = 0\n",
        "CNNnet = CNNBert(768)\n",
        "CNNnet.to(CNNgpu)   \n",
        "CNNopti = torch.optim.AdamW(CNNnet.parameters(), lr=2e-5, weight_decay=0.97)\n",
        "CNNcriterion = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuKjfc5_aFzM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def CNNtrain(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "\n",
        "    total_steps = len(train_loader) * max_eps\n",
        "    scheduler = get_linear_schedule_with_warmup(opti, \n",
        "                                        num_warmup_steps = 0,\n",
        "                                        num_training_steps = total_steps)\n",
        "    for ep in range(max_eps):\n",
        "        net.train()\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq = seq.cuda(gpu)\n",
        "            attn_masks = attn_masks.cuda(gpu)\n",
        "            labels = labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            loss = criterion(logits, labels.unsqueeze(1).float())\n",
        "            loss.backward()\n",
        "            opti.step()\n",
        "            \n",
        "            if it % 200 == 0:\n",
        "                print(\"Iteration {} of epoch {} Time taken (s): {}, loss =  {}\".format(it, ep, (time.time()-st),loss.item()))\n",
        "                st = time.time() \n",
        "\n",
        "CNNtrain(CNNnet, CNNcriterion, CNNopti, overall_loader, dev_loader, 4, CNNgpu)            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqaQ8MAQf6BQ"
      },
      "source": [
        "### LSTM / LSTM with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQT7qOezPbPO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfDXd4BnPd5e"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
        "tokenizer.fit_on_texts(X_train_feature3)\n",
        "\n",
        "y_train = np.array(y_train, dtype=np.int32)\n",
        "y_dev = np.array(y_dev, dtype=np.int32)\n",
        "y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "x_train = tokenizer.texts_to_matrix(X_train_feature3, mode=\"count\")\n",
        "\n",
        "vocab_size = x_train.shape[1]\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(X_train_feature3)\n",
        "x_dev = tokenizer.texts_to_sequences(X_dev_feature3)\n",
        "x_test = tokenizer.texts_to_sequences(X_test_feature3)\n",
        "\n",
        "maxlen = 30\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_dev = pad_sequences(x_dev, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "print(\"Vocab size =\", vocab_size)\n",
        "print(vocab_size)\n",
        "print(len(y_train))\n",
        "\n",
        "embedding_dim = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDogqPntPoeq"
      },
      "outputs": [],
      "source": [
        "# pure LSTM\n",
        "model = Sequential(name=\"lstm\")\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))\n",
        "model.add(LSTM(10))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(x_train, y_train, epochs=1, batch_size=2)\n",
        "\n",
        "loss, accuracy = model.evaluate(x_dev, y_dev, verbose=False)\n",
        "\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0-q6qJoPpAg"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, bias=True):\n",
        "        super(LSTM, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=False, bidirectional=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(hidden_dim,1)\n",
        "    \n",
        "    def forward(self, seq, attn_masks):\n",
        "        bert_output = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
        "        cont_reps = bert_output.last_hidden_state\n",
        "        lstm_output, _ = self.lstm(cont_reps)\n",
        "        lstm_out = lstm_output.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(lstm_out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = out.view(BATCHSIZE, -1)\n",
        "        out = out[:,-1]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_7DaDJtQEuY"
      },
      "outputs": [],
      "source": [
        "input_dim = 768\n",
        "hidden_dim = 100\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        " \n",
        "model = LSTM(input_dim, hidden_dim, num_layers, output_dim)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "lstmGPU = 0\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "train(model, criterion, optimizer, train_loader, dev_loader, 4, lstmGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSmpAQ-Hggm2"
      },
      "source": [
        "## evaluation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7v3TzhLaF1f"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, loader):\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, label in loader:\n",
        "            seq, attn_masks = seq.cuda(gpu), attn_masks.cuda(gpu)\n",
        "            logits = model(seq, attn_masks)\n",
        "            probs = logits.cpu().numpy().flatten()\n",
        "            soft_probs = [int(prob > 0.5) for prob in probs]\n",
        "            predictions += soft_probs\n",
        "    return predictions\n",
        "\n",
        "def Evaluationfile(model,dev_loader):\n",
        "    pre = get_predictions(model,dev_loader)\n",
        "    with open(\"my_dev_pre_lable.txt\",'w',newline='') as f:\n",
        "        csvwritter = csv.writer(f,delimiter = ',')\n",
        "        for prediction in pre:\n",
        "            csvwritter.writerow([\"rumour\"]) if prediction == 1 else csvwritter.writerow([\"nonrumour\"])\n",
        "    \n",
        "    acutallabels = BertDEVDataset.LABEL\n",
        "    with open(\"my_dev_actual_lable.txt\",'w',newline='') as f2:\n",
        "        csvwritter = csv.writer(f2,delimiter = ',')\n",
        "        for acutallabel in acutallabels:\n",
        "            csvwritter.writerow([\"rumour\"]) if acutallabel == 1 else csvwritter.writerow([\"nonrumour\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "COMP90042A3T1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
